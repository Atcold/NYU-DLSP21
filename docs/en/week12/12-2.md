---
lang-ref: ch.12-2
title: Low Resource Machine Translation
lecturer: Marc’Aurelio Ranzato
authors: Angela Teng, Joanna Jin
date: 17 May 2021
---

# 12.2
## Wed 21 Apr – (Invited) Lecture 12: Guest Lecture by Marc'Aurelio Ranzato

This week's lecture was a guest lecture by [Marc'Aurelio Ranzato](https://ai.facebook.com/people/marc-aurelio-ranzato/), who is a research scientist and manager at the Facebook AI Research (FAIR) lab, where he works to enable machines to learn with weaker supervision and to efficiently transfer knowledge across tasks. The first part of this document focuses on understanding low resource machine translation, and the second half discusses potential domain mismatches in machine learning and machine translation.

## ML Perspective on Low Resource MT
We start with standard machine learning algorithms can be applied in the realm of machine translation translation. Then, we take a deeper look into understanding different perspectives on this application. Our setting consists of multiple languages and multiple domains, but eventually we want to maximize translation accuracy of a certain domain in a language class.

<center>
<img src="{​{site.baseurl}}/images/week12/12-2/figure1.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
<!-- <b>Figure 1:</b> Network architecture. -->
</center>
<!-- ![](https://i.imgur.com/p6HO305.png) -->

Taking a deeper look at the type of data we are presented with in machine translation, we can better understand how these applications can be mapped to machine learning techniques. For example, if we have a parallel dataset in the space of machine translation, then we have its equivalen in supervised learning. Second, we could have monolingual data in machine translation--this translates to semi-supervised learning within the machine elarning framework. Third, if we have multiple language paris in machine translation, we can closely compare this to multi-task learning within the ML space. And lastly, if we have multiple domains in machine translation, we can compare this to domain adaptation in machine learning. When you have many domains, you naturally also want to use different domain adaptation techniques


## Case Studies
**Translating from English to Nepali:** Let's start with a simple case study within the realm of supervsied learning. For example, let's say that we have a sentence in English and we want to trasnslate it to Nepali. This is similar to multitask learning in the sense that you have one task, and then you add another task you're interested in. Since we have multiple domains we can start thinking about domain adaptation techniques and analyzing which domain adaptation techniques are applicable to machine translation.


<center>
<img src="{​{site.baseurl}}/images/week12/12-2/figure2.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
<!-- <b>Figure 2:</b> Network architecture. -->
</center>

Beginning with a supervised learning setting, we can use the aforementioned example of translation. Our dataset is $D = \{(x,y)_i\}_{i=1, ..., N}$ We can train this model using maximum likelihood, where we want to maximize $\{y|x\}$.

One way to represent this methodology is by the diagram, where you have a blue encoder that processes English sentences, and a red decoder that processes Nepali. From here, we want to compute our cross-entropy loss and update our model parameters. We may also want to regularize the model using either dropout or label smoothing. Additionally, we may need to regularize using the log loss $-lo gp(y|x;\theta)$

*Note: Transliteration is not word per word translation. It means that you're using the charaters from one language to make a logical translation of the word in a different language.*

**How can we improve generalization?**

<center>
<img src="{​{site.baseurl}}/images/week12/12-2/figure3.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
<!-- <b>Figure 3:</b> Network architecture. -->
</center>

One way to leverage an additional dataset is by trying to model $p(x)$ with a semi-supervised approach. One way to model $p(x)$ is via the denoising of an auto-encoder. This is particualrly useful because the encoder and decoder share a simlar machine translation methodology.

<center>
<img src="{​{site.baseurl}}/images/week12/12-2/figure4.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
<!-- <b>Figure 4:</b> Network architecture. -->
</center>

Another approach would be via self-training or pseudolearning. This is an algorithm from the 90s and the idea is that you take your sentence from the monolingual dataset and then you inject noise to it, you encode the poor translation you made and then make a prediction to produce the desired outcome. You can tune the parameters by minimizing the standard cross entropy loss on the labeled data. In other words, we are minimizing $L^{sup} (\theta) + \lambda L^{ST} \theta$. We are basically using a stale version of our model to produce the desired output $y$.

<center>
<img src="{​{site.baseurl}}/images/week12/12-2/figure5.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
<!-- <b>Figure 5:</b> Network architecture. -->
</center>

This method works because:
    (1) when we produce $y$, we typically trying to learn the search procedure
    (2) we insert noise which creates smoothing for the output space

**Working with monolingual data:** If we are working on the other hand with monolingual data, first we would need to train a reverse machine learning translation system of backward machines.

<center>
<img src="{​{site.baseurl}}/images/week12/12-2/figure6.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
<!-- <b>Figure 1:</b> Network architecture. -->
</center>

The algorithm would stay the same as above, where we have smaller systems of encoders and decoders.

<center>
<img src="{​{site.baseurl}}/images/week12/12-2/figure7.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
<!-- <b>Figure 1:</b> Network architecture. -->
</center>

Finally, we can put these two algorithms together in an iterative process.

<center>
<img src="{​{site.baseurl}}/images/week12/12-2/figure8.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
<!-- <b>Figure 1:</b> Network architecture. -->
</center>

Next question is how to deal with multiple languages

<center>
<img src="{​{site.baseurl}}/images/week12/12-2/figure9.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
<!-- <b>Figure 1:</b> Network architecture. -->
</center>

This is the learning framework for multilingul training, which shares encoder and decoder across all the language pairs, prepends a target language identifier to the source sentence to inform decoder of desired language and concatenates all the datasets together.

Mathematically, the train uses standard cross-entropy loss:

$\mathcal{L}(\theta)=-\sum_{s,t} \mathbb{E}_{(x,y)\sim\mathcal{D}}[log p(y|x,t)]$

#### How do we deal with domain adaptation?

If you have small data in domain validation, what you can do is fine-tuning, which is super effective. Basically train on domain A and finetune on domain B by continuing training for a little bit on the validation set.

### Unsupervised MT
Let's consider English and French.
<center>
<img src="{​{site.baseurl}}/images/week12/12-2/figure10.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
<!-- <b>Figure 1:</b> Network architecture. -->
</center>


#### Iterative BT
 The following architecture first translate French to some random unknown English words. Since there is no ground truth reference, what could do is feeding this translation to another machine translation system goes from English to French. The problem here is lack of constrains on $\bar{x}$

 <center>
 <img src="{​{site.baseurl}}/images/week12/12-2/figure11.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
 <!-- <b>Figure 1:</b> Network architecture. -->
 </center>



#### DAE
DAE adds a constraint on the $\bar{x}$ to make sure that decoder outputs fluently in the desired language. One way is adding denoising of the encoding turn to the loss function. This may not work since decoder may behave differently when fed with representations from French encoder vs English encoder (lack of modularity)

<center>
<img src="{​{site.baseurl}}/images/week12/12-2/figure12.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
<!-- <b>Figure 1:</b> Network architecture. -->
</center>


#### Multi-Lingual
One way to fix is by sharing all parameters of encoder and deconder so that the feature space share no matter whether you feed the French or English sentence (only one encoder and decoder now)

<center>
<img src="{​{site.baseurl}}/images/week12/12-2/figure13.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
<!-- <b>Figure 1:</b> Network architecture. -->
</center>

#### BLEU score

<center>
<img src="{​{site.baseurl}}/images/week12/12-2/figure14.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
<!-- <b>Figure 1:</b> Network architecture. -->
</center>


This graph tells why machine translation is very large scale learning, because you need to compensate for the lack of direct supervision by adding more and more data.

### FLoRes Ne-En

<center>
<img src="{​{site.baseurl}}/images/week12/12-2/figure15.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
<!-- <b>Figure 1:</b> Network architecture. -->
</center>

#### Results on FLoRes: Ne-En
<center>
<img src="{​{site.baseurl}}/images/week12/12-2/figure16.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
<!-- <b>Figure 1:</b> Network architecture. -->
</center>

The unsupervised learning doesn't work at all in this case because two monolingual for the stats are from different domains and there is no way to find corresponences.

If adding English-Hindi data since Hindi and Nepali are similar, all four models dramatically improve (Red).

If you want to add language that is less related, you need to add so much. Since low-resource MT requires big data and big compute!

In conclusion, the less labeled data you have, the more data you need to use:

<center>
<img src="{​{site.baseurl}}/images/week12/12-2/figure17.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
<!-- <b>Figure 1:</b> Network architecture. -->
</center>


### Perspectives
Typically, in machine learning and machine translation, we always consider a domain mismatch between the training and the test distribution. When we have a slightly different domain, we need to do domain adaptation (e.g. domain tagging and finetuning). Here we will talk about a different domain mismach which is:

#### Source-Target Domain Mismatch
<center>
<img src="{​{site.baseurl}}/images/week12/12-2/figure18.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
<!-- <b>Figure 1:</b> Network architecture. -->
</center>

#### How to quantify STDM?
<center>
<img src="{​{site.baseurl}}/images/week12/12-2/figure19.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
<!-- <b>Figure 1:</b> Network architecture. -->
</center>

#### In conclusion:
* STDM: a new kind of domain mismatch, intrinsic to the MT task.
* STDM is particularly significant in low resource language pairs.
* Metric & controlled setting enable study and better understanding of STDM.
* Methods that leverage source side monolingual data are more robust to STDM.
* In practice, the influence of STDM depends on several factors, such as the amount of
parallel and monolingual data, the domains, language pair, etc. In particular, if domains
are not too distinct, STDM may even help regularizing!


Prepared by Angela Teng (part 1) and Joanna Jin (part 2)
