---
lang: fr
lang-ref: ch.02-3
title: Motivation des problèmes, algèbre linéaire et visualisation
lecturer: Alfredo Canziani
authors: Rajashekar Vasantha
date: 04 Feb 2021
typora-root-url: 02-3
translation-date: 19 Jun 2021
translator: Loïck Bourdois
---


<!--
## Resources

Please follow Alfredo Canziani [on Twitter @alfcnz](https://twitter.com/alfcnz). Videos and textbooks with relevant details on linear algebra and singular value decomposition (SVD) can be found by searching Alfredo's Twitter, for example type `linear algebra (from:alfcnz)` in the search box.
-->
## Ressources

Nous vous invitons à suivre Alfredo Canziani [sur Twitter @alfcnz](https://twitter.com/alfcnz). Vous trouverez sur son compte des vidéos et des manuels contenant des détails pertinents sur l'algèbre linéaire et la décomposition en valeurs singulières (SVD). Ce contenu est trouvable en effectuant une recherche (en anglais) sur le Twitter d'Alfredo, en tapant par exemple `linear algebra (from:alfcnz)` dans la barre de recherche.


<!--
## [Neural Nets: Rotation and Squashing](https://youtu.be/0TdAmZUMj2k)
A traditional neural network is an alternating collection of two blocks - the linear blocks and the non-linear blocks. Given below is a block diagram of a traditional neural network.
<br>
<br>
<center>
<img src="{{site.baseurl}}/images/week02/02-3/figure1.png" width="1000px"/>
Figure 1: Block Diagram of a Traditional Neural Network
</center>
<br>
The linear blocks (Rotations, for simplicity) are given by:

$$
\vect{s}_{k+1} = \mW_k z_k
$$

And the non-linear blocks (Squashing functions for intuitive understanding) are given by:

$$ \vect{z}_k = h(\vect{s}_k) $$

In the above diagram and equations, $$\vx \in \mathbb{R}^n$$ represents the input vector. $$\mW_k \in \mathbb{R}^{n_{k} \times n_{k-1}}$$ represents the matrix of an affine transformation corresponding to the $$k^{\text{th}}$$ block and is described below in further detail. The function $h$ is called the activation function and this function forms the non-linear block of the neural network. Sigmoid, ReLu and tanh are some of the common activation functions and we will look at them in the later parts of this section. After alternate applications of linear and non-linear blocks, the above network produces an output vector $$\vect{s}_k \in \mathbb{R}^{n_{k-1}}$$.

Let us first have a look at the linear block to gain some intuition on affine transformations. As a motivating example, let us consider image classification. Suppose we take a picture with a 1 megapixel camera. This image will have about 1,000 pixels vertically and 1,000 pixels horizontally, and each pixel will have three colour dimensions for red, green, and blue (RGB). Each particular image can then be considered as one point in a 3 million-dimensional space. With such massive dimensionality, many interesting images we might want to classify -- such as a dog *vs.* a cat -- will essentially be in the same region of the space.

In order to effectively separate these images, we consider ways of transforming the data in order to move the points. Recall that in 2-D space, a linear transformation is the same as matrix multiplication. For example, the following are transformations, which can be obtained by changing matrix characterictics:

-   Rotation (when the matrix is orthonormal).
-   Scaling (when the matrix is diagonal).
-   Reflection (when the determinant is negative).
-   Shearing.
-   Translation.

Note that translation alone is not linear since 0 will not always be mapped to 0, but it is an affine transformation. Returning to our image example, we can transform the data points by translating such that the points are clustered around 0 and scaling with a diagonal matrix such that we "zoom in" to that region. Finally, we can do classification by finding lines across the space which separate the different points into their respective classes. In other words, the idea is to use linear and nonlinear transformations to map the points into a space such that they are linearly separable. This idea will be made more concrete in the following sections.

In the next part, we visualize how a neural network separates points and a few linear and non-linear transformations. This can be accessed [here](https://atcold.github.io/NYU-DLSP20/en/week01/01-3/).
-->


## [Réseaux neuronaux : rotation et écrasement](https://youtu.be/0TdAmZUMj2k)
Un réseau de neurones traditionnel est une collection alternée de deux blocs : les blocs linéaires et les blocs non linéaires.
Voici le schéma fonctionnel d'un réseau de neurones traditionnel.
<br>
<br>
<center>
<img src="{{site.baseurl}}/images/week02/02-3/figure1.png" width="1000px"/>
  <b>Figure 1 :</b> Schéma d'un réseau de neurones traditionnel
</center>
<br>
Les blocs linéaires (rotations pour simplifier) sont donnés par :

$$
\vect{s}_{k+1} = \mW_k z_k
$$

Et les blocs non linéaires (fonctions d'écrasement pour une compréhension intuitive) sont donnés par :


$$ \vect{z}_k = h(\vect{s}_k) $$

Dans le schéma et les équations ci-dessus,  $$\vx \in \mathbb{R}^n$$ représente le vecteur d'entrée.
$$\mW_k \in \mathbb{R}^{n_{k} \times n_{k-1}}$$ représente la matrice d'une transformation affine correspondant au $$k^{\text{ème}}$$ bloc et est décrite plus en détail ci-dessous.
La fonction $h$ est appelée fonction d'activation et cette fonction forme le bloc non linéaire du réseau neuronal.
Sigmoïde, ReLU et tanh sont quelques-unes des fonctions d'activation les plus courantes et nous les examinerons dans les parties suivantes de cette section.
Après des applications alternées des blocs linéaire et non linéaire, le réseau ci-dessus produit un vecteur de sortie $$\vect{s}_k \in \mathbb{R}^{n_{k-1}}$$.

Examinons d'abord le bloc linéaire pour comprendre les transformations affines. Comme exemple considérons la classification d'images.
Supposons que nous prenions une photo avec un appareil photo de $1$ mégapixel.
Cette image aura environ $1 000$ pixels verticalement et $1 000$ pixels horizontalement, et chaque pixel aura trois dimensions de couleur pour le rouge, le vert et le bleu (RVB).
Chaque image peut donc être considérée comme un point dans un espace à $3$ millions de dimensions.
Avec une telle dimensionnalité, de nombreuses images intéressantes que nous pourrions vouloir classer, comme un chien *vs* un chat, se trouveront essentiellement dans la même région de l'espace.

Afin de séparer efficacement ces images, nous envisageons des moyens de transformer les données afin de déplacer les points.
Rappelons que dans l'espace bidimensionnel, une transformation linéaire équivaut à une multiplication de matrice.
Par exemple, les transformations suivantes peuvent être obtenues en changeant les caractéristiques de la matrice :

- Rotation : lorsque la matrice est orthonormée.
- Mise à l'échelle (« scalabilité ») : lorsque la matrice est diagonale.
- Réflexion : lorsque le déterminant est négatif.
- *Shearing*.
- Translation.

A noter que la translation seule n'est pas linéaire puisque $0$ ne sera pas toujours mis en correspondance avec 0, mais c'est une transformation affine.
Pour revenir à notre exemple d'image, nous pouvons transformer les points de données en les translatant de manière à ce qu'ils soient regroupés autour de 0 et en les mettant à l'échelle à l'aide d'une matrice diagonale de manière à effectuer un « zoom avant » sur cette région.
Enfin, nous pouvons effectuer une classification en trouvant des lignes dans l'espace qui séparent les différents points dans leurs classes respectives.
En d'autres termes, l'idée est d'utiliser des transformations linéaires et non linéaires pour représenter les points dans un espace tel qu'ils soient linéairement séparables.
Cette idée sera rendue plus concrète dans les sections suivantes.

Dans la suite, nous visualisons comment un réseau neuronal sépare des points et quelques transformations linéaires et non linéaires.
Ce contenu est essentiellement le même que celui de l'année dernière, ainsi nous vous invitons à vous rendre [ici](https://atcold.github.io/NYU-DLSP20/fr/week01/01-3/) pour le consulter.
